{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:43.606968Z",
     "start_time": "2020-03-17T15:22:43.602972Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:43.671788Z",
     "start_time": "2020-03-17T15:22:43.608956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., nan,  2., nan,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array([1, np.nan, 2, np.nan, 3, 4, 5])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:43.697721Z",
     "start_time": "2020-03-17T15:22:43.673783Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:43.732626Z",
     "start_time": "2020-03-17T15:22:43.700711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.142857142857143"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.nan_to_num(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:43.761548Z",
     "start_time": "2020-03-17T15:22:43.734620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:43.794460Z",
     "start_time": "2020-03-17T15:22:43.765538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., nan,  2., nan,  3.,  4.,  5.],\n",
       "       [nan, nan, nan, nan, nan, nan, nan],\n",
       "       [ 2., nan,  4., nan,  6.,  8., 10.],\n",
       "       [nan, nan, nan, nan, nan, nan, nan],\n",
       "       [ 3., nan,  6., nan,  9., 12., 15.],\n",
       "       [ 4., nan,  8., nan, 12., 16., 20.],\n",
       "       [ 5., nan, 10., nan, 15., 20., 25.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(arr, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:55.604122Z",
     "start_time": "2020-03-17T15:22:43.796454Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master=\"local[2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:55.880384Z",
     "start_time": "2020-03-17T15:22:55.607116Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:22:55.888363Z",
     "start_time": "2020-03-17T15:22:55.883377Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:02.284431Z",
     "start_time": "2020-03-17T15:22:55.894354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count :  168398\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ny_df = sqlContext.read.parquet(\"../Data/NY.parquet/\")\n",
    "print(\"Total count : \", ny_df.count())\n",
    "ny_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:02.476685Z",
     "start_time": "2020-03-17T15:23:02.285422Z"
    }
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(ny_df, \"new_york_weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:04.447919Z",
     "start_time": "2020-03-17T15:23:02.477187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|measurement|count|\n",
      "+-----------+-----+\n",
      "|   TOBS_s20|10956|\n",
      "|       TOBS|10956|\n",
      "|       TMAX|13437|\n",
      "|   TMAX_s20|13437|\n",
      "|       TMIN|13442|\n",
      "|   TMIN_s20|13442|\n",
      "|   SNWD_s20|14617|\n",
      "|       SNWD|14617|\n",
      "|       SNOW|15629|\n",
      "|   SNOW_s20|15629|\n",
      "|   PRCP_s20|16118|\n",
      "|       PRCP|16118|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT measurement, COUNT(measurement) AS count\n",
    "FROM new_york_weather\n",
    "GROUP BY measurement\n",
    "ORDER BY count\n",
    "\"\"\"\n",
    "\n",
    "counts = sqlContext.sql(query)\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:04.472852Z",
     "start_time": "2020-03-17T15:23:04.450910Z"
    }
   },
   "outputs": [],
   "source": [
    "# principle component analysis - pca\n",
    "\n",
    "def outer_product(x):\n",
    "    \"\"\"compute outer product and indicate which locations in the matrix are undefined\"\"\"\n",
    "    outer_res = np.outer(x, x)\n",
    "    nan_count = 1 - np.isnan(outer_res)\n",
    "    return (outer_res, nan_count)\n",
    "\n",
    "def sum_with_nan(m1, m2):\n",
    "    \"\"\"add two pairs and return (matrix, count)\"\"\"\n",
    "    (x1, n1) = m1\n",
    "    (x2, n2) = m2\n",
    "    n = n1 + n2\n",
    "    x = np.nansum(np.dstack((x1, x2)), axis=2)\n",
    "    return (x, n)\n",
    "\n",
    "def data_func(sum_of_matrix, non_nan_num):\n",
    "    \"\"\"function to return sample data\"\"\"\n",
    "    # E -> sum of vectors\n",
    "    E = np.ones([365])\n",
    "    # NE -> number of not-nan entries for each coordinate of the vectors\n",
    "    NE = np.ones([365])\n",
    "    # MEAN -> mean of the vectors ignoring nans\n",
    "    MEAN = np.ones([365])\n",
    "    # O -> sum of the outer products\n",
    "    O = np.ones([365, 365])\n",
    "    # NO -> number of non-nans in the outer product\n",
    "    NO = np.ones([365, 365])\n",
    "    return E, NE, MEAN, O, NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:04.508756Z",
     "start_time": "2020-03-17T15:23:04.475843Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cov(RDD):\n",
    "    \"\"\"input -> input RDD of numpy arrays (all same length)\n",
    "       computes -> covariance matrix of the vectors\"\"\"\n",
    "    # insert 1 at the beginning of each vector so that the calculation also yields the mean vector\n",
    "    rdd = RDD.map(lambda v: np.array(np.insert(v, 0, 1), dtype=np.float64))\n",
    "    \n",
    "    outer_rdd = rdd.map(outer_product)\n",
    "    (S, N) = outer_rdd.reduce(sum_with_nan)\n",
    "    \n",
    "    E, NE, MEAN, O, NO = data_func(S, N)\n",
    "    \n",
    "    cov_matrix = O / NO - np.outer(MEAN, MEAN)\n",
    "    var = np.array([cov_matrix[i, i] for i in range(cov_matrix.shape[0])])\n",
    "    return {\n",
    "        \"E\": E,\n",
    "        \"NE\": NE,\n",
    "        \"O\": O,\n",
    "        \"NO\": NO,\n",
    "        \"COV\": cov_matrix,\n",
    "        \"MEAN\": MEAN,\n",
    "        \"VAR\": var\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:06.145762Z",
     "start_time": "2020-03-17T15:23:04.515738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigen value = \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0.]\n",
      "eigen vector = \n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "v = 2 * (np.random.random([2, 10]) - 0.5)\n",
    "# print(\"vector : \", v)\n",
    "\n",
    "data_list = []\n",
    "for i in range(1000):\n",
    "    f = 2 * (np.random.random(2) - 0.5)\n",
    "    data_list.append(np.dot(f, v))\n",
    "\n",
    "# compute covariance matrix\n",
    "rdd = sc.parallelize(data_list)\n",
    "out = compute_cov(rdd)\n",
    "\n",
    "# find PCA decomposition\n",
    "eig_val, eig_vec = linalg.eig(out[\"COV\"])\n",
    "print(\"eigen value = \\n\", eig_val)\n",
    "print(\"eigen vector = \\n\", eig_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:06.152745Z",
     "start_time": "2020-03-17T15:23:06.148755Z"
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:06.186663Z",
     "start_time": "2020-03-17T15:23:06.155738Z"
    }
   },
   "outputs": [],
   "source": [
    "def pack_array(arr):\n",
    "    \"\"\"pack a numpy array into a byte-array\"\"\"\n",
    "    if type(arr) != np.ndarray:\n",
    "        raise Exception(\"input for pack_array should be numpy.ndarray, instead got {}.\".format(str(type(arr))))\n",
    "    return bytearray(arr.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:06.218570Z",
     "start_time": "2020-03-17T15:23:06.190644Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpack_array(arr):\n",
    "    \"\"\"unpack a byte-array to numpy.ndarray\"\"\"\n",
    "    return np.frombuffer(arr, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:23:06.265442Z",
     "start_time": "2020-03-17T15:23:06.222561Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_percentiles(sorted_values, percentile):\n",
    "    L = int(len(sorted_values) / percentile)\n",
    "    return sorted_values[L], sorted_values[-L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:30:41.035607Z",
     "start_time": "2020-03-17T15:30:41.027629Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_overall_distribution(RDD):\n",
    "    \"\"\"compute the overall distribution of values and distribution of number of nan per year\"\"\"\n",
    "    undef_temp = RDD.map(lambda row: sum(np.isnan(row))).sample(False, 0.01).collect()\n",
    "    undefined = np.array(undef_temp)\n",
    "    flat = RDD.flatMap(lambda v: list(v)).filter(lambda x: not np.isnan(x)).cache()\n",
    "    count, s1, s2 = flat.map(lambda x: np.float64([1, x, x**2])).reduce(lambda x, y: x + y)\n",
    "    mean = s1 / count\n",
    "    std = np.sqrt(s2 / count - mean**2)\n",
    "    vals = flat.sample(False, 0.0001).collect()\n",
    "    sorted_values = np.array(sorted(vals))\n",
    "    low_100, high_100 = find_percentiles(sorted_values, 100)\n",
    "    low_1000, high_1000 = find_percentiles(sorted_values, 1000)\n",
    "    return {\n",
    "        \"undefined\": undefined,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"sorted_values\": sorted_values,\n",
    "        \"low_100\": low_100,\n",
    "        \"high_100\": high_100,\n",
    "        \"low_1000\": low_1000,\n",
    "        \"high_1000\": high_1000\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:30:41.490473Z",
     "start_time": "2020-03-17T15:30:41.482466Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_statistics(sqlContext, df):\n",
    "    \"\"\"compute all statistics for the data_frame\n",
    "       input -> sqlContext : perform SQL queries\n",
    "                data_frame : consists of below fields\n",
    "                    - Station (string)\n",
    "                    - Measurement (string)\n",
    "                    - Year (integer)\n",
    "                    - Values (byteArray with 365 float16 numbers)\n",
    "       returns -> STAT : dictionary of dictionaries\"\"\"\n",
    "    \n",
    "    sqlContext.registerDataFrameAsTable(df, \"weather\")\n",
    "    STAT = {}\n",
    "    measurements = ['TMAX', 'SNOW', 'SNWD', 'TMIN', 'PRCP', 'TOBS']\n",
    "    \n",
    "    for measure in measurements:\n",
    "        t = time()\n",
    "        query = \"SELECT * FROM weather WHERE measurement='%s'\" % (measure)\n",
    "        measure_df = sqlContext.sql(query)\n",
    "        print(\"{} : shape -> {}\".format(measure, measure_df.count()))\n",
    "        # unpack column -> values to float16\n",
    "        data = measure_df.rdd.map(lambda row: unpack_array(row[\"Values\"]))\n",
    "        # compute basic statistics\n",
    "        STAT[measure] = compute_overall_distribution(data)\n",
    "        # compute covariance matrix\n",
    "        out = compute_cov(data)\n",
    "        # compute pca decomposition\n",
    "        eig_val, eig_vec = linalg.eig(out[\"COV\"])\n",
    "        \n",
    "        # collect all statistics for measure\n",
    "        STAT[measure][\"eig_val\"] = eig_val\n",
    "        STAT[measure][\"eig_vec\"] = eig_vec\n",
    "        STAT[measure].update(out)\n",
    "        \n",
    "        print(\"time for {} is {}\".format(measure, time() - t))\n",
    "    \n",
    "    return STAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T15:46:00.406195Z",
     "start_time": "2020-03-17T15:30:41.965619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TMAX : shape -> 13437\n",
      "time for TMAX is 152.0520203113556\n",
      "SNOW : shape -> 15629\n",
      "time for SNOW is 169.7507836818695\n",
      "SNWD : shape -> 14617\n",
      "time for SNWD is 152.31755566596985\n",
      "TMIN : shape -> 13442\n",
      "time for TMIN is 153.168390750885\n",
      "PRCP : shape -> 16118\n",
      "time for PRCP is 173.7279713153839\n",
      "TOBS : shape -> 10956\n",
      "time for TOBS is 117.41092300415039\n",
      "Wall time: 15min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "STAT = compute_statistics(sqlContext, ny_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-17T17:38:15.592797Z",
     "start_time": "2020-03-17T17:38:15.534952Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_name = \"STAT_NY.pickle\"\n",
    "pickle.dump(STAT, open(file_name, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "144px",
    "left": "996px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
